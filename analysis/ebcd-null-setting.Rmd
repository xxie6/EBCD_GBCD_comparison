---
title: "ebcd-null-setting"
author: "Annie Xie"
date: "2025-04-24"
output: 
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---

# Introduction
In this example, I want to test out EBCD on a null dataset -- the data matrix $X$ is generated from normal noise and then $S = XX'$ is formed.

# Packages and Functions
```{r}
library(ebnm)
library(pheatmap)
library(ggplot2)
library(dplyr)
library(ebcd)
```

```{r}
plot_heatmap <- function(L, title = "", colors_range = c("gray96", "red"), brks = NULL){
  ### define the color map
  cols <- colorRampPalette(colors_range)(49)
  if (is.null(brks) == TRUE){
    brks <- seq(min(L), max(L), length=50)
  }
  
  plt <- pheatmap(L, show_rownames = FALSE, show_colnames = FALSE, cluster_rows = FALSE, cluster_cols = FALSE, color = cols, breaks = brks, main = title)
  return(plt)
}

plot_loadings <- function(L_est, Pop, legendYN = TRUE, scales_option = "fixed"){
  # options for scales_option are "fixed" and "free_y"
  n <- nrow(L_est)
  k <- ncol(L_est)
  Idx <- rep(c(1:n), k)
  Loading <- c(L_est)
  Factor <- paste0('k=',c(sapply(c(1:k), function(x, n){rep(x, n)}, n = n)))
  tib <- data.frame(Idx, Loading, Factor, Pop)
  plt <- ggplot(tib, aes(x = Idx, y = Loading, col = Pop)) +
    geom_point(show.legend = legendYN) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    facet_grid(cols = vars(Factor), scales = scales_option) +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          panel.spacing = unit(1, "lines"))
  plot(plt)
}
```

# Data Generation

```{r}
sim_null_data <- function(args) {
  set.seed(args$seed)
  
  Y <- matrix(rnorm(args$n * args$p, sd = args$indiv_sd), nrow = n)
  YYt <- (1/args$p)*tcrossprod(Y)
  
  return(list(Y = Y, YYt = YYt))
}
```

```{r}
n <- 100
p <- 1000
indiv_sd <- 1
seed <- 1
sim_args = list(n = n, p = p, indiv_sd = indiv_sd, seed = seed)
sim_data <- sim_null_data(sim_args)
```

This is a heatmap of the scaled Gram matrix:
```{r}
plot_heatmap(sim_data$YYt, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(sim_data$YYt)), max(abs(sim_data$YYt)), length.out = 50))
```

# EBCD

Here, I run EBCD with `Kmax` set to 100.
```{r}
set.seed(1)
ebcd_fit <- ebcd(X = t(sim_data$Y), Kmax = 100, ebnm_fn = ebnm::ebnm_point_exponential)
```

These are the number of columns in the estimate:
```{r}
ncol(ebcd_fit$EL)
```

This is a heatmap of the loadings estimate:
```{r}
plot_heatmap(ebcd_fit$EL, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fit$EL)), max(abs(ebcd_fit$EL)), length.out = 50))
```

This is a heatmap of $Z$ (untransformed):
```{r}
plot_heatmap(ebcd_fit$Z, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fit$Z)), max(abs(ebcd_fit$Z)), length.out = 50))
```

This is a scatter plot of the third column of $Z$ vs the third column of $L$:
```{r}
ggplot(data = NULL, aes(x = ebcd_fit$EL[,3], y = ebcd_fit$Z[,3])) + geom_point()
```

This is a heatmap of the $C$ matrix used during the method:
```{r}
plot_heatmap(ebcd_fit$A, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fit$A)), max(abs(ebcd_fit$A)), length.out = 50))
```

This is the estimate of $\tau$:
```{r}
ebcd_fit$tau
```

This is the L2 norm of the difference:
```{r}
sqrt(sum((ebcd_fit$A - tcrossprod(ebcd_fit$Z, ebcd_fit$EL))^2))
```

This is the objective function:
```{r}
ebcd_fit$obj
```

This is the fit term (up to a constant) of the objective function. Smaller is better.:
```{r}
ebcd_fit$tau*(sum((ebcd_fit$A - tcrossprod(ebcd_fit$Z, ebcd_fit$EL))^2))
```

This is the KL:
```{r}
sum(ebcd_fit$KL)
```

## Observations
EBCD adds 100 factors even though we are in the null setting. Looking at the loadings, it seems like each column has a notable non-zero element in only one of the samples. The estimate of $\tau$ is very large, so it seems like EBCD explained most of the variation with $L$ and $Z$. For comparison, I tried fitting GBCD and EBMFcov to this data (both with point-exponential priors), and both methods ultimately found a fit with 0 factors.

If EBCD had returned a fit with 0 factors, then the objective function value would be
```{r}
tau_est <- prod(dim(ebcd_fit$A)) / (sum((ebcd_fit$A)^2))
rank_zero_obj <- -ebcd_fit$N * ncol(ebcd_fit$A) / 2 * log(2 * pi / tau_est) +
      -(ebcd_fit$N * tau_est / 2) * (sum(ebcd_fit$A^2) / ebcd_fit$nrowA) 
rank_zero_obj
```

The fit term would be:
```{r}
tau_est*(sum(ebcd_fit$A)^2)
```

Therefore, the objective function prefers the fit with 100 factors. Furthermore, the fit term from the fit with 100 factors is much smaller (smaller is better) than that of the rank zero model. It is known that as $p$ increases, the influence of the prior decreases and EBCD will return the estimate that optimizes fit. So perhaps we are in that setting now.

Another observation is that the untransformed $Z$ matrix and $L$ matrix look very similar. This may be due to the symmetry of the $C$ matrix that is used in the method. Since the matrix is symmetric, perhaps that the method wants to find a factorization that is close to symmetric while still satisfying the orthogonality constraints and any constraints placed on the prior of $L$.

# Rank one EBCD
## Greedy Fit
Here, we greedily fit rank-one EBCD:
```{r}
set.seed(1)
ebcd_fit_init <- ebcd_init(X = t(sim_data$Y))
ebcd_greedy_fit <- ebcd_greedy(ebcd_fit_init, Kmax = 1, ebnm_fn = ebnm::ebnm_point_exponential)
```

This is a plot of $\ell$:
```{r}
plot(ebcd_greedy_fit$EL[,1])
```

## Backfit
Here, we backfit the rank-one fit:
```{r}
ebcd_backfit_fit <- ebcd_backfit(ebcd_greedy_fit)
```

This is a plot of $\ell$:
```{r}
plot(ebcd_backfit_fit$EL[,1])
```

## Progression of estimate
The greedy method is initialized with SVD
```{r}
set.seed(1)
svd1 <- irlba::irlba(ebcd_fit$A, nv = 1, nu = 0)
dv <- svd1$d * svd1$v
l <- dv / sqrt(ebcd_fit$nrowA)

estimates_list <- list(l)
for (i in 1:11){
  set.seed(1)
  estimates_list[[(i+1)]] <- ebcd_greedy(ebcd_fit_init, Kmax = 1, ebnm_fn = ebnm::ebnm_point_exponential, maxiter = i)$EL[,1]
}
```

These are scatter plots of the estimate at various iterations:
```{r}
par(mfrow = c(6,2), mar = c(2, 2, 1, 1) + 0.1)
max_y <- max(sapply(estimates_list, max))
min_y <- min(sapply(estimates_list, min))
for (i in 1:12){
  plot(estimates_list[[i]], main = paste('Iter', (i-1)), ylab = 'L', ylim = c(min_y, max_y))
}
par(mfrow = c(1,1))
```

This is a scatter plot of the initial estimate vs the estimate after 1 iteration:
```{r}
ggplot(data = NULL, aes(x = estimates_list[[1]], y = estimates_list[[2]])) + geom_point()
```

This is a scatter plot of the estimate after 1 iteration vs after 2 iterations:
```{r}
ggplot(data = NULL, aes(x = estimates_list[[2]], y = estimates_list[[3]])) + geom_point()
```

I also tried this with point-Laplace prior and found that the estimate looked very similar to the first eigenvector of the $C$ matrix.